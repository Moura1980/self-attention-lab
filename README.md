# LAB P1-01 â€” Scaled Dot-Product Attention

ImplementaÃ§Ã£o *from scratch* do mecanismo de Self-Attention.

```bash
â–¶ï¸ Como rodar
pip install numpy
python test_attention.py

ğŸ§  Como a normalizaÃ§Ã£o (âˆšdâ‚–) foi aplicada
ApÃ³s calcular o produto escalar Q @ K.T, o resultado Ã© dividido por:
scores_scaled = scores / np.sqrt(d_k)

ğŸ“Œ Softmax
O softmax Ã© aplicado em cada linha da matriz de scores:
attn_weights = self.softmax(scores_scaled, axis=1)
Cada linha passa a representar uma distribuiÃ§Ã£o de probabilidade (soma â‰ˆ 1).

ğŸ“Š Exemplo de entrada
Q = [[1,0], [0,1], [1,1]]
K = [[1,0], [0,1], [1,1]]
V = [[10,0], [0,10], [5,5]]

ğŸ“Š SaÃ­da esperada
Matriz de atenÃ§Ã£o 3x3 (cada linha soma ~1)
Output final 3x2 apÃ³s multiplicaÃ§Ã£o pelos valores V
